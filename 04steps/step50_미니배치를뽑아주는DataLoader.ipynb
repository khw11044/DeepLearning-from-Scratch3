{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "97ae724bfa85b9b34df7982b8bb8c7216f435b92902d749e4263f71162bea840"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Step 50, 미니배치를 뽑아주는 DataLoader\n",
    "\n",
    "이전 단계에서는 Dataset 클래스를 만들어서 통일된 인터페이스로 데이터셋을 다룰 수 있게 하였다.  \n",
    "이번 단계에서는 Dataset 클래스에서 미니배치를 뽑아주는 DataLoader 클래스를 구현한다.\n",
    "\n",
    "DataLoader는 미니배치 생성, 데이터셋 shuffle 등의 기능을 제공하여 사용자가 작성해야 할 학습 코드가 더 간단해진다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 50.1 반복자"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "t = [1,2,3]\n",
    "x = iter(t)\n",
    "next(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "next(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "next(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "StopIteration",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-92de4e9f6b1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "next(x)"
   ]
  },
  {
   "source": [
    "파이썬에서는 반복자를 직접 만들 수도 있다.  \n",
    "\n",
    "기본 반복자를 만들어보고 반복자 구조를 이용하여 미니배치를 뽑아주는 DataLoader 클래스를 구현해 볼것이다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 반복자\n",
    "class MyIterator:\n",
    "    def __init__(self, max_cnt):\n",
    "        self.max_cnt = max_cnt\n",
    "        self.cnt = 0\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.cnt == self.max_cnt:\n",
    "            raise StopIteration()\n",
    "\n",
    "        self.cnt += 1 \n",
    "        return self.cnt "
   ]
  },
  {
   "source": [
    "클래스를 파이썬 반복자로 사용하려면  \n",
    "\\__iter\\__ 특수 메서드를 구현, 자기 자신(self)을 반환  \n",
    "\\__next\\__ 특수 메서드 다음 원소를 반환하도록 구현"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n2\n3\n4\n5\n"
     ]
    }
   ],
   "source": [
    "obj = MyIterator(5)\n",
    "for x in obj:\n",
    "    print(x)"
   ]
  },
  {
   "source": [
    "다음으로 반복자 구조를 이용하여 미니배치를 뽑아주는 DataLoader 클래스를 구현  \n",
    "DataLoader는 주어진 데이터셋의 첫 데이터부터 차례로 꺼내주지만, 필요에 따라 shuffle 할 수도 있다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dezero/dataloaders.py \n",
    "\n",
    "import math \n",
    "import random\n",
    "import numpy as np \n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, dataset, batch_size, shuffle=True):\n",
    "        self.dataset = dataset          # Dataset 인터페이스를 만족하는 인스턴스\n",
    "        self.batch_size = batch_size    # 배치 크기\n",
    "        self.shuffle = shuffle          # 에포크별로 데이터셋을 shuffle 할지 여부\n",
    "        self.data_size = len(dataset)\n",
    "        self.max_iter = math.ceil(self.data_size / batch_size)\n",
    "\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.iteration = 0      # 반복 횟수 초기화 \n",
    "        if self.shuffle:\n",
    "            self.index = np.random.permutation(len(self.dataset))   # 데이터 shuffle\n",
    "        else:\n",
    "            self.index = np.arange(len(self.dataset))\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):         # 미니배치를 꺼내 ndarray 인스턴스로 변환\n",
    "        if self.iteration >= self.max_iter:\n",
    "            self.reset()\n",
    "            raise StopIteration\n",
    "\n",
    "        i = self.iteration\n",
    "        batch_size = self.batch_size\n",
    "        batch_index = self.index[i * batch_size : (i + 1) * batch_size]\n",
    "        batch = [self.dataset[i] for i in batch_index]\n",
    "        # 배치별로 나눔\n",
    "        x = np.array([example[0] for example in batch])\n",
    "        t = np.array([example[1] for example in batch])\n",
    "\n",
    "        self.iteration += 1\n",
    "        return x,t\n",
    "    \n",
    "    def next(self):\n",
    "        return self.__next__()"
   ]
  },
  {
   "source": [
    "마지막으로 dezero/\\__init\\__.py에 from dezero.dataloaders import DataLoader 추가 "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 50.2 DataLoader 사용하기 "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(10, 2) (10,)\n(10, 2) (10,)\n"
     ]
    }
   ],
   "source": [
    "from dezero.datasets import Spiral\n",
    "from dezero import DataLoader \n",
    "\n",
    "batch_size = 10\n",
    "max_epoch = 1\n",
    "\n",
    "train_set = Spiral(train=True)\n",
    "test_set = Spiral(train=False)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size, shuffle=False)   \n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    for x, t in train_loader:\n",
    "        print(x.shape, t.shape) # x,t는 train data\n",
    "        break \n",
    "\n",
    "    # epoch 끝에서 test data를 꺼낸다.\n",
    "    for x,t in test_loader:\n",
    "        print(x.shape, t.shape) # x, t는 test data \n",
    "        break "
   ]
  },
  {
   "source": [
    "## 50.3 accuracy 함수 구현하기 \n",
    "\n",
    "정확도를 평가해주는 accuracy 함수를 추가한다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dezero/functions.py \n",
    "from dezero import Variable, as_variable, as_array\n",
    "\n",
    "def accuracy(y, t):\n",
    "    y, t = as_variable(y), as_variable(t)\n",
    "\n",
    "    pred = y.data.argmax(axis=1).reshape(t.shape)\n",
    "    result = (pred == t.data)  # 텐서(ndarray)에 True/False 결과가 저장, \n",
    "    acc = result.mean()         # 이 텐서에 True 비율(평균)이 정답률\n",
    "    return Variable(as_array(acc))"
   ]
  },
  {
   "source": [
    "accuracy 함수는 인수 y와 t를 받아서 '정답률'을 계산  \n",
    "y : 신경망의 예측 결과   \n",
    "t : 정답 데이터\n",
    "\n",
    "pred : 신경망의 예측 결과를의 최대 인덱스를 찾아서 형상을 변경하여 저장\n",
    "pred와 정답 데이터 t를 비교하면 결과는 True/False의 텐서 (ndarray)가 된다.  \n",
    "이 텐서의 True 비율(평균)이 정답률에 해당"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 50.4 스파이럴 데이터셋 학습 코드\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ": 0.6667\n",
      "test loss: 0.6076, accuracy: 0.6967\n",
      "epoch: 63\n",
      "train loss: 0.5914, accuracy: 0.6700\n",
      "test loss: 0.6020, accuracy: 0.6300\n",
      "epoch: 64\n",
      "train loss: 0.5911, accuracy: 0.6867\n",
      "test loss: 0.5981, accuracy: 0.6533\n",
      "epoch: 65\n",
      "train loss: 0.5626, accuracy: 0.7000\n",
      "test loss: 0.6124, accuracy: 0.7100\n",
      "epoch: 66\n",
      "train loss: 0.5707, accuracy: 0.7067\n",
      "test loss: 0.5760, accuracy: 0.6567\n",
      "epoch: 67\n",
      "train loss: 0.5574, accuracy: 0.6900\n",
      "test loss: 0.5834, accuracy: 0.6967\n",
      "epoch: 68\n",
      "train loss: 0.5560, accuracy: 0.7133\n",
      "test loss: 0.5589, accuracy: 0.6933\n",
      "epoch: 69\n",
      "train loss: 0.5355, accuracy: 0.7267\n",
      "test loss: 0.5530, accuracy: 0.6967\n",
      "epoch: 70\n",
      "train loss: 0.5303, accuracy: 0.7300\n",
      "test loss: 0.5380, accuracy: 0.7267\n",
      "epoch: 71\n",
      "train loss: 0.5207, accuracy: 0.7167\n",
      "test loss: 0.5315, accuracy: 0.7367\n",
      "epoch: 72\n",
      "train loss: 0.5094, accuracy: 0.7533\n",
      "test loss: 0.5340, accuracy: 0.7367\n",
      "epoch: 73\n",
      "train loss: 0.5110, accuracy: 0.7400\n",
      "test loss: 0.5127, accuracy: 0.7433\n",
      "epoch: 74\n",
      "train loss: 0.4921, accuracy: 0.7500\n",
      "test loss: 0.5051, accuracy: 0.7400\n",
      "epoch: 75\n",
      "train loss: 0.4882, accuracy: 0.7700\n",
      "test loss: 0.5268, accuracy: 0.7467\n",
      "epoch: 76\n",
      "train loss: 0.4859, accuracy: 0.7533\n",
      "test loss: 0.4997, accuracy: 0.7400\n",
      "epoch: 77\n",
      "train loss: 0.4701, accuracy: 0.7767\n",
      "test loss: 0.4814, accuracy: 0.7800\n",
      "epoch: 78\n",
      "train loss: 0.4574, accuracy: 0.7733\n",
      "test loss: 0.4736, accuracy: 0.7700\n",
      "epoch: 79\n",
      "train loss: 0.4547, accuracy: 0.7533\n",
      "test loss: 0.4665, accuracy: 0.7900\n",
      "epoch: 80\n",
      "train loss: 0.4533, accuracy: 0.7667\n",
      "test loss: 0.4601, accuracy: 0.7900\n",
      "epoch: 81\n",
      "train loss: 0.4431, accuracy: 0.7967\n",
      "test loss: 0.4496, accuracy: 0.7900\n",
      "epoch: 82\n",
      "train loss: 0.4309, accuracy: 0.8167\n",
      "test loss: 0.4421, accuracy: 0.7867\n",
      "epoch: 83\n",
      "train loss: 0.4282, accuracy: 0.8167\n",
      "test loss: 0.4431, accuracy: 0.7833\n",
      "epoch: 84\n",
      "train loss: 0.4132, accuracy: 0.8167\n",
      "test loss: 0.4353, accuracy: 0.8200\n",
      "epoch: 85\n",
      "train loss: 0.4132, accuracy: 0.8167\n",
      "test loss: 0.4260, accuracy: 0.8367\n",
      "epoch: 86\n",
      "train loss: 0.4047, accuracy: 0.8167\n",
      "test loss: 0.4248, accuracy: 0.8400\n",
      "epoch: 87\n",
      "train loss: 0.3968, accuracy: 0.8367\n",
      "test loss: 0.4031, accuracy: 0.8333\n",
      "epoch: 88\n",
      "train loss: 0.3942, accuracy: 0.8267\n",
      "test loss: 0.4091, accuracy: 0.8333\n",
      "epoch: 89\n",
      "train loss: 0.3746, accuracy: 0.8333\n",
      "test loss: 0.3962, accuracy: 0.8100\n",
      "epoch: 90\n",
      "train loss: 0.3772, accuracy: 0.8333\n",
      "test loss: 0.3831, accuracy: 0.8300\n",
      "epoch: 91\n",
      "train loss: 0.3669, accuracy: 0.8467\n",
      "test loss: 0.3844, accuracy: 0.8433\n",
      "epoch: 92\n",
      "train loss: 0.3554, accuracy: 0.8600\n",
      "test loss: 0.3752, accuracy: 0.8567\n",
      "epoch: 93\n",
      "train loss: 0.3491, accuracy: 0.8700\n",
      "test loss: 0.3720, accuracy: 0.8400\n",
      "epoch: 94\n",
      "train loss: 0.3440, accuracy: 0.8533\n",
      "test loss: 0.3595, accuracy: 0.8600\n",
      "epoch: 95\n",
      "train loss: 0.3408, accuracy: 0.8633\n",
      "test loss: 0.3533, accuracy: 0.8700\n",
      "epoch: 96\n",
      "train loss: 0.3336, accuracy: 0.8600\n",
      "test loss: 0.3515, accuracy: 0.8800\n",
      "epoch: 97\n",
      "train loss: 0.3295, accuracy: 0.8700\n",
      "test loss: 0.3437, accuracy: 0.8933\n",
      "epoch: 98\n",
      "train loss: 0.3208, accuracy: 0.9133\n",
      "test loss: 0.3433, accuracy: 0.8533\n",
      "epoch: 99\n",
      "train loss: 0.3212, accuracy: 0.8667\n",
      "test loss: 0.3302, accuracy: 0.8633\n",
      "epoch: 100\n",
      "train loss: 0.3146, accuracy: 0.8700\n",
      "test loss: 0.3248, accuracy: 0.8600\n",
      "epoch: 101\n",
      "train loss: 0.3090, accuracy: 0.8633\n",
      "test loss: 0.3268, accuracy: 0.8833\n",
      "epoch: 102\n",
      "train loss: 0.3024, accuracy: 0.8933\n",
      "test loss: 0.3195, accuracy: 0.8533\n",
      "epoch: 103\n",
      "train loss: 0.3009, accuracy: 0.8933\n",
      "test loss: 0.3122, accuracy: 0.8900\n",
      "epoch: 104\n",
      "train loss: 0.2930, accuracy: 0.9067\n",
      "test loss: 0.3087, accuracy: 0.8633\n",
      "epoch: 105\n",
      "train loss: 0.2881, accuracy: 0.9067\n",
      "test loss: 0.3111, accuracy: 0.8567\n",
      "epoch: 106\n",
      "train loss: 0.2785, accuracy: 0.9167\n",
      "test loss: 0.3026, accuracy: 0.8867\n",
      "epoch: 107\n",
      "train loss: 0.2786, accuracy: 0.8933\n",
      "test loss: 0.2952, accuracy: 0.8833\n",
      "epoch: 108\n",
      "train loss: 0.2756, accuracy: 0.9100\n",
      "test loss: 0.2912, accuracy: 0.8800\n",
      "epoch: 109\n",
      "train loss: 0.2744, accuracy: 0.9200\n",
      "test loss: 0.2965, accuracy: 0.8667\n",
      "epoch: 110\n",
      "train loss: 0.2705, accuracy: 0.9267\n",
      "test loss: 0.2856, accuracy: 0.8933\n",
      "epoch: 111\n",
      "train loss: 0.2645, accuracy: 0.9067\n",
      "test loss: 0.2814, accuracy: 0.8867\n",
      "epoch: 112\n",
      "train loss: 0.2624, accuracy: 0.9100\n",
      "test loss: 0.2835, accuracy: 0.8700\n",
      "epoch: 113\n",
      "train loss: 0.2608, accuracy: 0.9067\n",
      "test loss: 0.2752, accuracy: 0.8933\n",
      "epoch: 114\n",
      "train loss: 0.2524, accuracy: 0.9167\n",
      "test loss: 0.2740, accuracy: 0.9033\n",
      "epoch: 115\n",
      "train loss: 0.2484, accuracy: 0.9167\n",
      "test loss: 0.2688, accuracy: 0.8967\n",
      "epoch: 116\n",
      "train loss: 0.2480, accuracy: 0.9200\n",
      "test loss: 0.2768, accuracy: 0.8600\n",
      "epoch: 117\n",
      "train loss: 0.2445, accuracy: 0.9133\n",
      "test loss: 0.2635, accuracy: 0.9067\n",
      "epoch: 118\n",
      "train loss: 0.2444, accuracy: 0.9267\n",
      "test loss: 0.2593, accuracy: 0.8967\n",
      "epoch: 119\n",
      "train loss: 0.2379, accuracy: 0.9267\n",
      "test loss: 0.2576, accuracy: 0.8867\n",
      "epoch: 120\n",
      "train loss: 0.2361, accuracy: 0.9200\n",
      "test loss: 0.2573, accuracy: 0.9200\n",
      "epoch: 121\n",
      "train loss: 0.2372, accuracy: 0.9333\n",
      "test loss: 0.2534, accuracy: 0.9067\n",
      "epoch: 122\n",
      "train loss: 0.2305, accuracy: 0.9333\n",
      "test loss: 0.2511, accuracy: 0.8867\n",
      "epoch: 123\n",
      "train loss: 0.2277, accuracy: 0.9367\n",
      "test loss: 0.2531, accuracy: 0.9000\n",
      "epoch: 124\n",
      "train loss: 0.2318, accuracy: 0.9233\n",
      "test loss: 0.2456, accuracy: 0.9067\n",
      "epoch: 125\n",
      "train loss: 0.2248, accuracy: 0.9400\n",
      "test loss: 0.2454, accuracy: 0.9133\n",
      "epoch: 126\n",
      "train loss: 0.2229, accuracy: 0.9433\n",
      "test loss: 0.2427, accuracy: 0.9033\n",
      "epoch: 127\n",
      "train loss: 0.2264, accuracy: 0.9167\n",
      "test loss: 0.2433, accuracy: 0.9000\n",
      "epoch: 128\n",
      "train loss: 0.2152, accuracy: 0.9333\n",
      "test loss: 0.2373, accuracy: 0.9133\n",
      "epoch: 129\n",
      "train loss: 0.2176, accuracy: 0.9367\n",
      "test loss: 0.2402, accuracy: 0.8967\n",
      "epoch: 130\n",
      "train loss: 0.2149, accuracy: 0.9200\n",
      "test loss: 0.2332, accuracy: 0.9033\n",
      "epoch: 131\n",
      "train loss: 0.2136, accuracy: 0.9333\n",
      "test loss: 0.2377, accuracy: 0.8933\n",
      "epoch: 132\n",
      "train loss: 0.2143, accuracy: 0.9200\n",
      "test loss: 0.2304, accuracy: 0.9167\n",
      "epoch: 133\n",
      "train loss: 0.2141, accuracy: 0.9267\n",
      "test loss: 0.2449, accuracy: 0.8833\n",
      "epoch: 134\n",
      "train loss: 0.2088, accuracy: 0.9133\n",
      "test loss: 0.2362, accuracy: 0.8900\n",
      "epoch: 135\n",
      "train loss: 0.2046, accuracy: 0.9333\n",
      "test loss: 0.2278, accuracy: 0.9133\n",
      "epoch: 136\n",
      "train loss: 0.2021, accuracy: 0.9333\n",
      "test loss: 0.2275, accuracy: 0.9200\n",
      "epoch: 137\n",
      "train loss: 0.2046, accuracy: 0.9333\n",
      "test loss: 0.2236, accuracy: 0.9167\n",
      "epoch: 138\n",
      "train loss: 0.1991, accuracy: 0.9267\n",
      "test loss: 0.2207, accuracy: 0.9233\n",
      "epoch: 139\n",
      "train loss: 0.1960, accuracy: 0.9433\n",
      "test loss: 0.2216, accuracy: 0.9267\n",
      "epoch: 140\n",
      "train loss: 0.1981, accuracy: 0.9333\n",
      "test loss: 0.2178, accuracy: 0.9233\n",
      "epoch: 141\n",
      "train loss: 0.1945, accuracy: 0.9367\n",
      "test loss: 0.2194, accuracy: 0.9200\n",
      "epoch: 142\n",
      "train loss: 0.1969, accuracy: 0.9233\n",
      "test loss: 0.2158, accuracy: 0.9233\n",
      "epoch: 143\n",
      "train loss: 0.1942, accuracy: 0.9333\n",
      "test loss: 0.2127, accuracy: 0.9167\n",
      "epoch: 144\n",
      "train loss: 0.1886, accuracy: 0.9433\n",
      "test loss: 0.2288, accuracy: 0.8933\n",
      "epoch: 145\n",
      "train loss: 0.1966, accuracy: 0.9133\n",
      "test loss: 0.2103, accuracy: 0.9167\n",
      "epoch: 146\n",
      "train loss: 0.1899, accuracy: 0.9300\n",
      "test loss: 0.2124, accuracy: 0.9100\n",
      "epoch: 147\n",
      "train loss: 0.1890, accuracy: 0.9400\n",
      "test loss: 0.2104, accuracy: 0.9200\n",
      "epoch: 148\n",
      "train loss: 0.1865, accuracy: 0.9333\n",
      "test loss: 0.2080, accuracy: 0.9267\n",
      "epoch: 149\n",
      "train loss: 0.1825, accuracy: 0.9367\n",
      "test loss: 0.2285, accuracy: 0.8967\n",
      "epoch: 150\n",
      "train loss: 0.1890, accuracy: 0.9233\n",
      "test loss: 0.2152, accuracy: 0.9067\n",
      "epoch: 151\n",
      "train loss: 0.1828, accuracy: 0.9400\n",
      "test loss: 0.2042, accuracy: 0.9300\n",
      "epoch: 152\n",
      "train loss: 0.1807, accuracy: 0.9467\n",
      "test loss: 0.2136, accuracy: 0.9067\n",
      "epoch: 153\n",
      "train loss: 0.1795, accuracy: 0.9333\n",
      "test loss: 0.2076, accuracy: 0.9033\n",
      "epoch: 154\n",
      "train loss: 0.1797, accuracy: 0.9300\n",
      "test loss: 0.2052, accuracy: 0.9200\n",
      "epoch: 155\n",
      "train loss: 0.1816, accuracy: 0.9400\n",
      "test loss: 0.2015, accuracy: 0.9233\n",
      "epoch: 156\n",
      "train loss: 0.1791, accuracy: 0.9300\n",
      "test loss: 0.2012, accuracy: 0.9333\n",
      "epoch: 157\n",
      "train loss: 0.1791, accuracy: 0.9367\n",
      "test loss: 0.1978, accuracy: 0.9267\n",
      "epoch: 158\n",
      "train loss: 0.1751, accuracy: 0.9400\n",
      "test loss: 0.2046, accuracy: 0.9033\n",
      "epoch: 159\n",
      "train loss: 0.1719, accuracy: 0.9433\n",
      "test loss: 0.2039, accuracy: 0.9067\n",
      "epoch: 160\n",
      "train loss: 0.1730, accuracy: 0.9433\n",
      "test loss: 0.1963, accuracy: 0.9300\n",
      "epoch: 161\n",
      "train loss: 0.1690, accuracy: 0.9467\n",
      "test loss: 0.1973, accuracy: 0.9267\n",
      "epoch: 162\n",
      "train loss: 0.1676, accuracy: 0.9567\n",
      "test loss: 0.1951, accuracy: 0.9400\n",
      "epoch: 163\n",
      "train loss: 0.1734, accuracy: 0.9433\n",
      "test loss: 0.1941, accuracy: 0.9300\n",
      "epoch: 164\n",
      "train loss: 0.1725, accuracy: 0.9233\n",
      "test loss: 0.1922, accuracy: 0.9367\n",
      "epoch: 165\n",
      "train loss: 0.1665, accuracy: 0.9433\n",
      "test loss: 0.1951, accuracy: 0.9233\n",
      "epoch: 166\n",
      "train loss: 0.1698, accuracy: 0.9433\n",
      "test loss: 0.1907, accuracy: 0.9333\n",
      "epoch: 167\n",
      "train loss: 0.1737, accuracy: 0.9300\n",
      "test loss: 0.1923, accuracy: 0.9300\n",
      "epoch: 168\n",
      "train loss: 0.1661, accuracy: 0.9367\n",
      "test loss: 0.1944, accuracy: 0.9300\n",
      "epoch: 169\n",
      "train loss: 0.1635, accuracy: 0.9567\n",
      "test loss: 0.1894, accuracy: 0.9233\n",
      "epoch: 170\n",
      "train loss: 0.1688, accuracy: 0.9400\n",
      "test loss: 0.1878, accuracy: 0.9300\n",
      "epoch: 171\n",
      "train loss: 0.1675, accuracy: 0.9300\n",
      "test loss: 0.1899, accuracy: 0.9367\n",
      "epoch: 172\n",
      "train loss: 0.1602, accuracy: 0.9500\n",
      "test loss: 0.1859, accuracy: 0.9400\n",
      "epoch: 173\n",
      "train loss: 0.1644, accuracy: 0.9433\n",
      "test loss: 0.1901, accuracy: 0.9267\n",
      "epoch: 174\n",
      "train loss: 0.1597, accuracy: 0.9500\n",
      "test loss: 0.1878, accuracy: 0.9267\n",
      "epoch: 175\n",
      "train loss: 0.1599, accuracy: 0.9400\n",
      "test loss: 0.1840, accuracy: 0.9367\n",
      "epoch: 176\n",
      "train loss: 0.1612, accuracy: 0.9533\n",
      "test loss: 0.1909, accuracy: 0.9200\n",
      "epoch: 177\n",
      "train loss: 0.1582, accuracy: 0.9500\n",
      "test loss: 0.1829, accuracy: 0.9433\n",
      "epoch: 178\n",
      "train loss: 0.1599, accuracy: 0.9433\n",
      "test loss: 0.1847, accuracy: 0.9333\n",
      "epoch: 179\n",
      "train loss: 0.1541, accuracy: 0.9400\n",
      "test loss: 0.1892, accuracy: 0.9267\n",
      "epoch: 180\n",
      "train loss: 0.1539, accuracy: 0.9500\n",
      "test loss: 0.1814, accuracy: 0.9367\n",
      "epoch: 181\n",
      "train loss: 0.1572, accuracy: 0.9400\n",
      "test loss: 0.1960, accuracy: 0.9333\n",
      "epoch: 182\n",
      "train loss: 0.1612, accuracy: 0.9400\n",
      "test loss: 0.1824, accuracy: 0.9367\n",
      "epoch: 183\n",
      "train loss: 0.1533, accuracy: 0.9533\n",
      "test loss: 0.1836, accuracy: 0.9233\n",
      "epoch: 184\n",
      "train loss: 0.1526, accuracy: 0.9533\n",
      "test loss: 0.1787, accuracy: 0.9433\n",
      "epoch: 185\n",
      "train loss: 0.1530, accuracy: 0.9533\n",
      "test loss: 0.1892, accuracy: 0.9100\n",
      "epoch: 186\n",
      "train loss: 0.1558, accuracy: 0.9433\n",
      "test loss: 0.1840, accuracy: 0.9267\n",
      "epoch: 187\n",
      "train loss: 0.1559, accuracy: 0.9400\n",
      "test loss: 0.1775, accuracy: 0.9367\n",
      "epoch: 188\n",
      "train loss: 0.1513, accuracy: 0.9500\n",
      "test loss: 0.1775, accuracy: 0.9367\n",
      "epoch: 189\n",
      "train loss: 0.1491, accuracy: 0.9533\n",
      "test loss: 0.1850, accuracy: 0.9267\n",
      "epoch: 190\n",
      "train loss: 0.1488, accuracy: 0.9500\n",
      "test loss: 0.1790, accuracy: 0.9333\n",
      "epoch: 191\n",
      "train loss: 0.1499, accuracy: 0.9467\n",
      "test loss: 0.1755, accuracy: 0.9400\n",
      "epoch: 192\n",
      "train loss: 0.1469, accuracy: 0.9433\n",
      "test loss: 0.1788, accuracy: 0.9267\n",
      "epoch: 193\n",
      "train loss: 0.1528, accuracy: 0.9433\n",
      "test loss: 0.1769, accuracy: 0.9367\n",
      "epoch: 194\n",
      "train loss: 0.1482, accuracy: 0.9433\n",
      "test loss: 0.1737, accuracy: 0.9467\n",
      "epoch: 195\n",
      "train loss: 0.1425, accuracy: 0.9533\n",
      "test loss: 0.1770, accuracy: 0.9333\n",
      "epoch: 196\n",
      "train loss: 0.1493, accuracy: 0.9433\n",
      "test loss: 0.1757, accuracy: 0.9300\n",
      "epoch: 197\n",
      "train loss: 0.1436, accuracy: 0.9467\n",
      "test loss: 0.1752, accuracy: 0.9300\n",
      "epoch: 198\n",
      "train loss: 0.1449, accuracy: 0.9433\n",
      "test loss: 0.1721, accuracy: 0.9433\n",
      "epoch: 199\n",
      "train loss: 0.1456, accuracy: 0.9467\n",
      "test loss: 0.1734, accuracy: 0.9300\n",
      "epoch: 200\n",
      "train loss: 0.1447, accuracy: 0.9500\n",
      "test loss: 0.1710, accuracy: 0.9433\n",
      "epoch: 201\n",
      "train loss: 0.1426, accuracy: 0.9500\n",
      "test loss: 0.1707, accuracy: 0.9433\n",
      "epoch: 202\n",
      "train loss: 0.1406, accuracy: 0.9600\n",
      "test loss: 0.1733, accuracy: 0.9400\n",
      "epoch: 203\n",
      "train loss: 0.1388, accuracy: 0.9667\n",
      "test loss: 0.1760, accuracy: 0.9333\n",
      "epoch: 204\n",
      "train loss: 0.1427, accuracy: 0.9433\n",
      "test loss: 0.1696, accuracy: 0.9467\n",
      "epoch: 205\n",
      "train loss: 0.1403, accuracy: 0.9567\n",
      "test loss: 0.1754, accuracy: 0.9433\n",
      "epoch: 206\n",
      "train loss: 0.1436, accuracy: 0.9400\n",
      "test loss: 0.1698, accuracy: 0.9433\n",
      "epoch: 207\n",
      "train loss: 0.1410, accuracy: 0.9567\n",
      "test loss: 0.1723, accuracy: 0.9333\n",
      "epoch: 208\n",
      "train loss: 0.1418, accuracy: 0.9567\n",
      "test loss: 0.1685, accuracy: 0.9400\n",
      "epoch: 209\n",
      "train loss: 0.1408, accuracy: 0.9467\n",
      "test loss: 0.1695, accuracy: 0.9367\n",
      "epoch: 210\n",
      "train loss: 0.1382, accuracy: 0.9533\n",
      "test loss: 0.1676, accuracy: 0.9467\n",
      "epoch: 211\n",
      "train loss: 0.1359, accuracy: 0.9633\n",
      "test loss: 0.1815, accuracy: 0.9367\n",
      "epoch: 212\n",
      "train loss: 0.1377, accuracy: 0.9600\n",
      "test loss: 0.1677, accuracy: 0.9433\n",
      "epoch: 213\n",
      "train loss: 0.1350, accuracy: 0.9467\n",
      "test loss: 0.1669, accuracy: 0.9467\n",
      "epoch: 214\n",
      "train loss: 0.1340, accuracy: 0.9600\n",
      "test loss: 0.1727, accuracy: 0.9367\n",
      "epoch: 215\n",
      "train loss: 0.1425, accuracy: 0.9567\n",
      "test loss: 0.1654, accuracy: 0.9467\n",
      "epoch: 216\n",
      "train loss: 0.1404, accuracy: 0.9533\n",
      "test loss: 0.1685, accuracy: 0.9333\n",
      "epoch: 217\n",
      "train loss: 0.1349, accuracy: 0.9567\n",
      "test loss: 0.1688, accuracy: 0.9367\n",
      "epoch: 218\n",
      "train loss: 0.1410, accuracy: 0.9533\n",
      "test loss: 0.1678, accuracy: 0.9300\n",
      "epoch: 219\n",
      "train loss: 0.1342, accuracy: 0.9533\n",
      "test loss: 0.1663, accuracy: 0.9400\n",
      "epoch: 220\n",
      "train loss: 0.1361, accuracy: 0.9500\n",
      "test loss: 0.1649, accuracy: 0.9433\n",
      "epoch: 221\n",
      "train loss: 0.1361, accuracy: 0.9500\n",
      "test loss: 0.1632, accuracy: 0.9533\n",
      "epoch: 222\n",
      "train loss: 0.1318, accuracy: 0.9633\n",
      "test loss: 0.1661, accuracy: 0.9433\n",
      "epoch: 223\n",
      "train loss: 0.1345, accuracy: 0.9567\n",
      "test loss: 0.1629, accuracy: 0.9467\n",
      "epoch: 224\n",
      "train loss: 0.1319, accuracy: 0.9600\n",
      "test loss: 0.1629, accuracy: 0.9533\n",
      "epoch: 225\n",
      "train loss: 0.1334, accuracy: 0.9533\n",
      "test loss: 0.1620, accuracy: 0.9500\n",
      "epoch: 226\n",
      "train loss: 0.1339, accuracy: 0.9633\n",
      "test loss: 0.1675, accuracy: 0.9367\n",
      "epoch: 227\n",
      "train loss: 0.1364, accuracy: 0.9467\n",
      "test loss: 0.1613, accuracy: 0.9467\n",
      "epoch: 228\n",
      "train loss: 0.1319, accuracy: 0.9533\n",
      "test loss: 0.1616, accuracy: 0.9467\n",
      "epoch: 229\n",
      "train loss: 0.1301, accuracy: 0.9600\n",
      "test loss: 0.1640, accuracy: 0.9300\n",
      "epoch: 230\n",
      "train loss: 0.1298, accuracy: 0.9467\n",
      "test loss: 0.1656, accuracy: 0.9333\n",
      "epoch: 231\n",
      "train loss: 0.1289, accuracy: 0.9567\n",
      "test loss: 0.1633, accuracy: 0.9500\n",
      "epoch: 232\n",
      "train loss: 0.1276, accuracy: 0.9600\n",
      "test loss: 0.1632, accuracy: 0.9433\n",
      "epoch: 233\n",
      "train loss: 0.1269, accuracy: 0.9567\n",
      "test loss: 0.1675, accuracy: 0.9333\n",
      "epoch: 234\n",
      "train loss: 0.1311, accuracy: 0.9600\n",
      "test loss: 0.1630, accuracy: 0.9433\n",
      "epoch: 235\n",
      "train loss: 0.1335, accuracy: 0.9500\n",
      "test loss: 0.1602, accuracy: 0.9467\n",
      "epoch: 236\n",
      "train loss: 0.1275, accuracy: 0.9633\n",
      "test loss: 0.1596, accuracy: 0.9500\n",
      "epoch: 237\n",
      "train loss: 0.1227, accuracy: 0.9633\n",
      "test loss: 0.1619, accuracy: 0.9467\n",
      "epoch: 238\n",
      "train loss: 0.1275, accuracy: 0.9600\n",
      "test loss: 0.1588, accuracy: 0.9433\n",
      "epoch: 239\n",
      "train loss: 0.1270, accuracy: 0.9533\n",
      "test loss: 0.1657, accuracy: 0.9367\n",
      "epoch: 240\n",
      "train loss: 0.1251, accuracy: 0.9500\n",
      "test loss: 0.1583, accuracy: 0.9433\n",
      "epoch: 241\n",
      "train loss: 0.1273, accuracy: 0.9533\n",
      "test loss: 0.1589, accuracy: 0.9500\n",
      "epoch: 242\n",
      "train loss: 0.1268, accuracy: 0.9567\n",
      "test loss: 0.1584, accuracy: 0.9500\n",
      "epoch: 243\n",
      "train loss: 0.1242, accuracy: 0.9633\n",
      "test loss: 0.1580, accuracy: 0.9433\n",
      "epoch: 244\n",
      "train loss: 0.1232, accuracy: 0.9633\n",
      "test loss: 0.1642, accuracy: 0.9433\n",
      "epoch: 245\n",
      "train loss: 0.1247, accuracy: 0.9600\n",
      "test loss: 0.1562, accuracy: 0.9500\n",
      "epoch: 246\n",
      "train loss: 0.1273, accuracy: 0.9567\n",
      "test loss: 0.1555, accuracy: 0.9500\n",
      "epoch: 247\n",
      "train loss: 0.1305, accuracy: 0.9500\n",
      "test loss: 0.1552, accuracy: 0.9533\n",
      "epoch: 248\n",
      "train loss: 0.1274, accuracy: 0.9500\n",
      "test loss: 0.1579, accuracy: 0.9467\n",
      "epoch: 249\n",
      "train loss: 0.1282, accuracy: 0.9500\n",
      "test loss: 0.1589, accuracy: 0.9500\n",
      "epoch: 250\n",
      "train loss: 0.1286, accuracy: 0.9600\n",
      "test loss: 0.1548, accuracy: 0.9533\n",
      "epoch: 251\n",
      "train loss: 0.1251, accuracy: 0.9667\n",
      "test loss: 0.1566, accuracy: 0.9400\n",
      "epoch: 252\n",
      "train loss: 0.1213, accuracy: 0.9533\n",
      "test loss: 0.1615, accuracy: 0.9167\n",
      "epoch: 253\n",
      "train loss: 0.1232, accuracy: 0.9633\n",
      "test loss: 0.1588, accuracy: 0.9433\n",
      "epoch: 254\n",
      "train loss: 0.1238, accuracy: 0.9567\n",
      "test loss: 0.1564, accuracy: 0.9400\n",
      "epoch: 255\n",
      "train loss: 0.1212, accuracy: 0.9633\n",
      "test loss: 0.1571, accuracy: 0.9533\n",
      "epoch: 256\n",
      "train loss: 0.1261, accuracy: 0.9600\n",
      "test loss: 0.1546, accuracy: 0.9433\n",
      "epoch: 257\n",
      "train loss: 0.1238, accuracy: 0.9600\n",
      "test loss: 0.1575, accuracy: 0.9533\n",
      "epoch: 258\n",
      "train loss: 0.1224, accuracy: 0.9633\n",
      "test loss: 0.1583, accuracy: 0.9333\n",
      "epoch: 259\n",
      "train loss: 0.1239, accuracy: 0.9533\n",
      "test loss: 0.1534, accuracy: 0.9467\n",
      "epoch: 260\n",
      "train loss: 0.1288, accuracy: 0.9567\n",
      "test loss: 0.1522, accuracy: 0.9500\n",
      "epoch: 261\n",
      "train loss: 0.1245, accuracy: 0.9567\n",
      "test loss: 0.1530, accuracy: 0.9500\n",
      "epoch: 262\n",
      "train loss: 0.1225, accuracy: 0.9600\n",
      "test loss: 0.1549, accuracy: 0.9433\n",
      "epoch: 263\n",
      "train loss: 0.1193, accuracy: 0.9533\n",
      "test loss: 0.1523, accuracy: 0.9533\n",
      "epoch: 264\n",
      "train loss: 0.1177, accuracy: 0.9633\n",
      "test loss: 0.1553, accuracy: 0.9433\n",
      "epoch: 265\n",
      "train loss: 0.1140, accuracy: 0.9633\n",
      "test loss: 0.1566, accuracy: 0.9500\n",
      "epoch: 266\n",
      "train loss: 0.1210, accuracy: 0.9500\n",
      "test loss: 0.1507, accuracy: 0.9533\n",
      "epoch: 267\n",
      "train loss: 0.1224, accuracy: 0.9567\n",
      "test loss: 0.1511, accuracy: 0.9500\n",
      "epoch: 268\n",
      "train loss: 0.1234, accuracy: 0.9500\n",
      "test loss: 0.1504, accuracy: 0.9533\n",
      "epoch: 269\n",
      "train loss: 0.1163, accuracy: 0.9567\n",
      "test loss: 0.1511, accuracy: 0.9500\n",
      "epoch: 270\n",
      "train loss: 0.1191, accuracy: 0.9567\n",
      "test loss: 0.1507, accuracy: 0.9533\n",
      "epoch: 271\n",
      "train loss: 0.1187, accuracy: 0.9567\n",
      "test loss: 0.1543, accuracy: 0.9367\n",
      "epoch: 272\n",
      "train loss: 0.1182, accuracy: 0.9633\n",
      "test loss: 0.1567, accuracy: 0.9267\n",
      "epoch: 273\n",
      "train loss: 0.1137, accuracy: 0.9567\n",
      "test loss: 0.1490, accuracy: 0.9533\n",
      "epoch: 274\n",
      "train loss: 0.1205, accuracy: 0.9467\n",
      "test loss: 0.1541, accuracy: 0.9400\n",
      "epoch: 275\n",
      "train loss: 0.1159, accuracy: 0.9633\n",
      "test loss: 0.1508, accuracy: 0.9467\n",
      "epoch: 276\n",
      "train loss: 0.1151, accuracy: 0.9633\n",
      "test loss: 0.1518, accuracy: 0.9533\n",
      "epoch: 277\n",
      "train loss: 0.1166, accuracy: 0.9633\n",
      "test loss: 0.1578, accuracy: 0.9233\n",
      "epoch: 278\n",
      "train loss: 0.1172, accuracy: 0.9667\n",
      "test loss: 0.1548, accuracy: 0.9400\n",
      "epoch: 279\n",
      "train loss: 0.1134, accuracy: 0.9633\n",
      "test loss: 0.1499, accuracy: 0.9533\n",
      "epoch: 280\n",
      "train loss: 0.1162, accuracy: 0.9533\n",
      "test loss: 0.1506, accuracy: 0.9500\n",
      "epoch: 281\n",
      "train loss: 0.1189, accuracy: 0.9700\n",
      "test loss: 0.1478, accuracy: 0.9567\n",
      "epoch: 282\n",
      "train loss: 0.1188, accuracy: 0.9600\n",
      "test loss: 0.1484, accuracy: 0.9533\n",
      "epoch: 283\n",
      "train loss: 0.1142, accuracy: 0.9667\n",
      "test loss: 0.1524, accuracy: 0.9433\n",
      "epoch: 284\n",
      "train loss: 0.1123, accuracy: 0.9700\n",
      "test loss: 0.1500, accuracy: 0.9433\n",
      "epoch: 285\n",
      "train loss: 0.1192, accuracy: 0.9600\n",
      "test loss: 0.1508, accuracy: 0.9467\n",
      "epoch: 286\n",
      "train loss: 0.1114, accuracy: 0.9600\n",
      "test loss: 0.1471, accuracy: 0.9533\n",
      "epoch: 287\n",
      "train loss: 0.1109, accuracy: 0.9633\n",
      "test loss: 0.1481, accuracy: 0.9567\n",
      "epoch: 288\n",
      "train loss: 0.1143, accuracy: 0.9533\n",
      "test loss: 0.1534, accuracy: 0.9467\n",
      "epoch: 289\n",
      "train loss: 0.1126, accuracy: 0.9600\n",
      "test loss: 0.1486, accuracy: 0.9500\n",
      "epoch: 290\n",
      "train loss: 0.1125, accuracy: 0.9733\n",
      "test loss: 0.1485, accuracy: 0.9533\n",
      "epoch: 291\n",
      "train loss: 0.1136, accuracy: 0.9700\n",
      "test loss: 0.1472, accuracy: 0.9500\n",
      "epoch: 292\n",
      "train loss: 0.1130, accuracy: 0.9600\n",
      "test loss: 0.1495, accuracy: 0.9433\n",
      "epoch: 293\n",
      "train loss: 0.1118, accuracy: 0.9700\n",
      "test loss: 0.1456, accuracy: 0.9533\n",
      "epoch: 294\n",
      "train loss: 0.1111, accuracy: 0.9600\n",
      "test loss: 0.1591, accuracy: 0.9367\n",
      "epoch: 295\n",
      "train loss: 0.1170, accuracy: 0.9533\n",
      "test loss: 0.1451, accuracy: 0.9533\n",
      "epoch: 296\n",
      "train loss: 0.1115, accuracy: 0.9567\n",
      "test loss: 0.1459, accuracy: 0.9567\n",
      "epoch: 297\n",
      "train loss: 0.1096, accuracy: 0.9667\n",
      "test loss: 0.1464, accuracy: 0.9533\n",
      "epoch: 298\n",
      "train loss: 0.1128, accuracy: 0.9500\n",
      "test loss: 0.1476, accuracy: 0.9533\n",
      "epoch: 299\n",
      "train loss: 0.1090, accuracy: 0.9633\n",
      "test loss: 0.1471, accuracy: 0.9467\n",
      "epoch: 300\n",
      "train loss: 0.1082, accuracy: 0.9667\n",
      "test loss: 0.1458, accuracy: 0.9600\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import dezero \n",
    "\n",
    "from dezero import optimizers\n",
    "from dezero import DataLoader\n",
    "from dezero.models import MLP\n",
    "import dezero.functions as F \n",
    "\n",
    "max_epoch = 300\n",
    "batch_size = 30 \n",
    "hidden_size = 10 \n",
    "lr = 1.0\n",
    "\n",
    "train_set = dezero.datasets.Spiral(train=True)\n",
    "test_set = dezero.datasets.Spiral(train=False)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size, shuffle=False)\n",
    "\n",
    "model = MLP((hidden_size, 3))\n",
    "optimizer = optimizers.SGD(lr).setup(model)\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    sum_loss, sum_acc = 0, 0\n",
    "\n",
    "    for x, t in train_loader:               # 1 훈련용 미니배치\n",
    "        y = model(x)\n",
    "        loss = F.softmax_cross_entropy(y,t)\n",
    "        acc = F.accuracy(y,t)               # 2 훈련 데이터의 인식 정확도 \n",
    "        model.cleargrads()\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "\n",
    "        sum_loss += float(loss.data) * len(t)\n",
    "        sum_acc += float(acc.data) * len(t)\n",
    "    \n",
    "    print('epoch: {}'.format(epoch+1))\n",
    "    print('train loss: {:.4f}, accuracy: {:.4f}'.format(\n",
    "        sum_loss / len(train_set), sum_acc / len(train_set)))\n",
    "    \n",
    "    sum_loss, sum_acc = 0, 0\n",
    "    with dezero.no_grad():          # 3 기울기 불필요 모드 : 테스트 데이터셋에 모델을 테스트만 하는거니깐\n",
    "        for x, t in test_loader:    # 4 테스트용 미니배치 데이터\n",
    "            y = model(x)\n",
    "            loss = F.softmax_cross_entropy(y, t)\n",
    "            acc = F.accuracy(y, t)  # 5 테스트 데이터의 인식 정확도 \n",
    "            sum_loss += float(loss.data) * len(t)\n",
    "            sum_acc += float(acc.data) * len(t)\n",
    "\n",
    "    print('test loss: {:.4f}, accuracy: {:.4f}'.format(\n",
    "        sum_loss / len(test_set), sum_acc / len(test_set)))"
   ]
  },
  {
   "source": [
    "**NOTE_** 과대적합(overfitting)은 특정 훈련 데이터에 지나치게 최적화된 상태를 말한다.  \n",
    "따라서 새로운 데이터에서는 예측 정확도가 훨씬 떨어지는, 일반화되지 못한 상태를 뜻한다.  \n",
    "신경망으로는 표현력이 높은 모델을 말들 수 있기 때문에 과대적합이 흔히 일어난다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}