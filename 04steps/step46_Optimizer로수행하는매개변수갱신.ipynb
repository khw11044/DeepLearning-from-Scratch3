{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "97ae724bfa85b9b34df7982b8bb8c7216f435b92902d749e4263f71162bea840"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# step46, Optimizer로 수행하는 매개변수 갱신\n",
    "\n",
    "지금까지 경사하강법으로 매개변수로 갱신하였다.  \n",
    "딥러닝 분야에서는 경사하강법 외에도 다양한 최적화 기법이 제안되고 있다.\n",
    "\n",
    "이번 단계에서는 매개변수 갱신 작업(갱신 코드)를 모듈화하고 쉽게 다른 모듈로 대체할 수 있는 구조를 만들어 본다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 46.1 Optimizer 클래스 \n",
    "\n",
    "매개변수 갱신을 위한 기반 클래스인 Optimizer를 구현한다.  \n",
    "Optimizer가 기초를 제공하고,  \n",
    "구체적인 최적화 기법은 Optimizer 클래스를 상속한 곳에서 구현한다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dezero/optimizers.py \n",
    "class Optimizer:\n",
    "    def __init__(self):\n",
    "        self.target = None \n",
    "        self.hooks = []\n",
    "\n",
    "    def setup(self, target):\n",
    "        self.target = target\n",
    "        return self \n",
    "    \n",
    "    def update(self):\n",
    "        # None 이외에 매개변수를 리스트에 모아둠 \n",
    "        params = [p for p in self.target.params() if p.grad is not None]\n",
    "\n",
    "        # 전처리(옵션)\n",
    "        for f in self.hooks:\n",
    "            f(params)\n",
    "        \n",
    "        # 매개변수 갱신 \n",
    "        for param in params:\n",
    "            self.update_one(param)\n",
    "    \n",
    "    def update_one(self, param):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def add_hook(self, f):\n",
    "        self.hooks.append(f)"
   ]
  },
  {
   "source": [
    "Optimizer 클래스의 초기화 메서드에서는 target과 hooks라는 두 개의 인스턴스 변수를 초기화  \n",
    "\n",
    "setup 메서드는 매개변수를 갖는 클래스(Model 또는 Layer)를 인스턴스 변수인 target으로 설정  \n",
    "\n",
    "update 메서드는 모든 매개변수를 갱신, 인스턴스 변수 grad가 None인 매개변수는 갱신을 건너뜀  \n",
    "\n",
    "update_one 메서드에서 구체적인 매개변수 갱신을 수행, 이 메서드를 Optimizer의 자식 클래스에서 재정의해야 한다.\n",
    "\n",
    "Optimizer 클래스는 전체 매개변수를 전처리해주는 기능도 갖췄다.  \n",
    "원하는 전처리가 있다면 ad_hook 메서들르 사용하여 전처리를 수행하는 함수를 추가한다.  \n",
    "이 구조 덕에 가중치 감소(Weight Decay)와 기울기 클리핑(Gradient Clipping)같은 기법을 이용할 수 있다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 46.2 SGD 클래스 구현\n",
    "\n",
    "SGD : Stochastic Gradient Descent, 확률적 경사 하강법  \n",
    "\n",
    "'확률적' : 대상 데이터 중에서 확률적으로 선별한 데이터에 대해 경사하강법을 수행한다는 뜻\n",
    "\n",
    "딥러닝에서는 원래의 데이터에서 무작위로 골라 경사하강법을 수행하는것이 일반적"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dezero/optimizers.py \n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, lr=0.01):\n",
    "        super().__init__()\n",
    "        self.lr = lr \n",
    "    \n",
    "    def update_one(self, param):\n",
    "        param.data -= self.lr * param.grad.data "
   ]
  },
  {
   "source": [
    "SGD 클래스는 Optimizer 클래스를 상속  \n",
    "\\__init\\__ 메서드는 학습률을 받아 초기화  \n",
    "update_one 메서드에서 매개변수 갱신 코드를 구현\n",
    "\n",
    "내생각 : 여기서 SGD가 거창한게 아니라 그냥 경사하강법을 Optimizer 클래스를 상속받어 구현할려고 예시처럼 넣어둔거 같다.  \n",
    "이후 나오는 다른 최적화 기법을 자세히 보면 되겠다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 46.3 SGD 클래스를 사용한 문제 해결 "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "variable(0.8165178492839196)\n",
      "variable(0.8165178492839196)\n",
      "variable(0.8165178492839196)\n",
      "variable(0.8165178492839196)\n",
      "variable(0.8165178492839196)\n",
      "variable(0.8165178492839196)\n",
      "variable(0.8165178492839196)\n",
      "variable(0.8165178492839196)\n",
      "variable(0.8165178492839196)\n",
      "variable(0.8165178492839196)\n"
     ]
    }
   ],
   "source": [
    "if '__file__' in globals():\n",
    "    import os, sys\n",
    "    sys.path.append(os.path.join(os.path.dirname(__file__), '..'))\n",
    "\n",
    "import numpy as np \n",
    "from dezero import Variable\n",
    "from dezero import optimizers\n",
    "import dezero.functions as F \n",
    "from dezero.models import MLP \n",
    "\n",
    "np.random.seed(0)\n",
    "x = np.random.rand(100,1)\n",
    "y = np.sin(2 * np.pi * x) + np.random.rand(100,1)\n",
    "\n",
    "lr = 0.2 \n",
    "max_iter = 10000 \n",
    "hidden_size = 10 \n",
    "\n",
    "model = MLP((hidden_size, 1))\n",
    "optimizer = optimizers.SGD(lr)\n",
    "optimizer.setup(model)\n",
    "\n",
    "for i in range(max_iter):\n",
    "    y_pred = model(x)\n",
    "    loss = F.mean_squared_error(y, y_pred)\n",
    "\n",
    "    model.cleargrads()\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.update \n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(loss)"
   ]
  },
  {
   "source": [
    "## 46.4 SGD 이외의 최적화 기법 \n",
    "\n",
    "Momentum, AdaGrad, AdaDelta, Adam등  \n",
    "Optimizer 클래스를 도입한 첫번째 목표는 다양한 최적화 기법을 필요에 따라 손쉽게 전환하기 위해서  \n",
    "그래서 기반 클래스인 Optimizer를 상속하여 다양한 최적화 기법을 구현해본다\n",
    "\n",
    "Momentum 기법의 수식 표현 \n",
    "\n",
    "$$v \\leftarrow \\alpha v - \\eta\\frac{\\partial L}{\\partial W}, \\qquad \\qquad \\qquad \\qquad (1)$$  \n",
    "\n",
    "$$W \\leftarrow W + v, \\qquad \\qquad \\qquad \\qquad  \\qquad (2)$$\n",
    "\n",
    "$W$는 갱신할 가중치 매개변수  \n",
    "$\\frac{\\partial L}{\\partial W}$는 기울기($W$에 관한 손실 함수 $L$의 기울기)  \n",
    "$\\eta$는 학습률을 뜻함  \n",
    "$v$는 물리에서 말하는 '속도'에 해당\n",
    "$\\alpha v$ 항은 물체가 아무런 힘을 받지 않을 떄 서서히 감속시키는 역할\n",
    "\n",
    "(1)은 물체가 기울기 방향으로 힘을 받아 가속되는 물리 법칙을 나타냄  \n",
    "(2)는 속도만큼 위치(매개변수)가 이동\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dezero/optimizers.py \n",
    "import numpy as np \n",
    "\n",
    "class MomentumSGD(Optimizer):\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        super().__init__()\n",
    "        self.lr = lr \n",
    "        self.momentum = momentum\n",
    "        self.vs = {}\n",
    "    \n",
    "    def update_one(self, param):\n",
    "        v_key = id(param)\n",
    "        if v_key not in self.vs:\n",
    "            self.vs[v_key] = np.zeros_like(param.data)\n",
    "        \n",
    "        v = self.vs[v_key]\n",
    "        v *= self.momentum\n",
    "        v -= self.lr * param.grad.data \n",
    "        param.data += v"
   ]
  },
  {
   "source": [
    "각 매개변수에는 '속도'에 해당하는 데이터가 있다. 이 데이터들을 딕셔너리 타입의 인스턴스 변수 self.vs에 유지  \n",
    "초기화 하는 사이에는 vs에 아무것도 담겨있지 않다.  \n",
    "update_one()이 호출될때 매개변수와 같은 타입의 데이터를 생성  \n",
    "그다음은 (1)과 (2)\n",
    "\n",
    "이제 손쉽게 Momentum으로 전환할 수 있다.  \n",
    "optimizer = SGD(lr)을 optimizer = MomentumSGD(lr)  \n",
    "로 바꿔주기만 하면 된다"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "optimizers.py에는 더 여러 최적화 기법들이 있다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dezero/optimizers.py \n",
    "class Adam(Optimizer):\n",
    "    def __init__(self, alpha=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.t = 0\n",
    "        self.alpha = alpha\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.ms = {}\n",
    "        self.vs = {}\n",
    "\n",
    "    def update(self, *args, **kwargs):\n",
    "        self.t += 1\n",
    "        super().update(*args, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def lr(self):\n",
    "        fix1 = 1. - math.pow(self.beta1, self.t)\n",
    "        fix2 = 1. - math.pow(self.beta2, self.t)\n",
    "        return self.alpha * math.sqrt(fix2) / fix1\n",
    "\n",
    "    def update_one(self, param):\n",
    "        xp = cuda.get_array_module(param.data)\n",
    "\n",
    "        key = id(param)\n",
    "        if key not in self.ms:\n",
    "            self.ms[key] = xp.zeros_like(param.data)\n",
    "            self.vs[key] = xp.zeros_like(param.data)\n",
    "\n",
    "        m, v = self.ms[key], self.vs[key]\n",
    "        beta1, beta2, eps = self.beta1, self.beta2, self.eps\n",
    "        grad = param.grad.data\n",
    "\n",
    "        m += (1 - beta1) * (grad - m)\n",
    "        v += (1 - beta2) * (grad * grad - v)\n",
    "        param.data -= self.lr * m / (xp.sqrt(v) + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}