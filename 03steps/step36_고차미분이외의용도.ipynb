{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "97ae724bfa85b9b34df7982b8bb8c7216f435b92902d749e4263f71162bea840"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Step36, 고차 미분 이외의 용도\n",
    "\n",
    "지금까지 한일 : 역전파 시 수행되는 계산에 대해서도 '연결'을 만들도록 했다.  \n",
    "역전파의 계산 그래프를 만드는 기능 자체가 DeZero의 새로운 능력이다.  \n",
    "고차 미분은 이 능력을 응용한 한 가지 예에 지나지 않는다.\n",
    "\n",
    "**NOTE_** 새로운 DeZero에서는 역전파로 수행한 계산에 대해 또 다른 역전파할 수 있다.  \n",
    "이를 double backpropagation이라고 한다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 36.1 double backprop의 용도 \n",
    "\n",
    "문제 : 다음의 두 식이 주어졌을 때 x=2.0에서 x에 대한 z의 미분 $\\frac{\\partial z}{\\partial x}$를 구하여라\n",
    "$$y=x^2$$\n",
    "$$z=\\bigg(\\frac{\\partial y}{\\partial x} \\bigg)^3+y$$ "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "variable(100.0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from dezero import Variable\n",
    "\n",
    "x = Variable(np.array(2.0))\n",
    "y = x**2 \n",
    "y.backward(create_graph=True)\n",
    "gx = x.grad\n",
    "x.cleargrad()\n",
    "\n",
    "z = gx**3 + y \n",
    "z.backward() \n",
    "print(x.grad)"
   ]
  },
  {
   "source": [
    "이 코드에서 중요한 부분은 y.backward(create_graph=True)이다.  \n",
    "미분을 하기 위해 역전파하는 코드이다.  \n",
    "이 코드가 새로운 계산 그래프를 생성한다.  \n",
    "\n",
    "그리고 역전파가 만들어낸 계산 그래프를 사용하여 새로운 계산을 하고 다시 역전파한다.\n",
    "\n",
    "_미분의 식을 구하고 그 식을 사용하여 계산한 다음 또 다시 미분하는 문제_를 double backprop로 해결할 수 있다.  \n",
    "딥러닝 연구에서도 많이 볼 수 있는 유형이다.\n",
    "\n",
    "**NOTE_**  \n",
    "gx=x.grad는 단순한 변수(값)가 아니라 계산 그래프(식)이다.  \n",
    "따라서 x.grad의 계산 그래프에 대해 추가로 역전파할 수 있다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 뉴턴방법과 double backprop 보충학습\n",
    "\n",
    "- 입력이 벡터인 경우의 뉴턴 방법 \n",
    "- 뉴턴 방법을 대체할 수 있는 또 다른 방법 \n",
    "- double backprop의 실용적인 쓰임 예\n",
    "\n",
    "## 다변수 함수의 뉴턴 방법 \n",
    "\n",
    "제 3 고지에서 뉴턴 방법을 구현하면서 $y=x^4 - 2x^2$라는 수식의 최솟값을 뉴턴 방법으로 구했다.  \n",
    ": '입력 변수가 하나(스칼라)인 경우의 뉴턴 방법을 구현'한 것이다.\n",
    "\n",
    "**입력이 다차원 배열인 경우 뉴턴 방법은?**  \n",
    "입력 변수를 벡터 **X**로 바꾸고 함수 $y=f(X)$  \n",
    "$X=(x_1,x_2,...,x_n)$ 형태로 n개의 원소를 갖는다.\n",
    "\n",
    "그러면 $y=f(x)$에 대한 뉴턴 방법  \n",
    "$$X \\leftarrow X - [\\nabla^2 f(x)]^{-1} \\nabla f'(x)$$\n",
    "\n",
    "$\\nabla f(x)$는 기울기를 나타낸다.  \n",
    "기울기는 X의 각 원소에 대한 미분\n",
    "\n",
    "$$\\nabla f(x) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix}$$\n",
    "\n",
    "$\\nabla^2 f(x)$는 헤세 행렬(Hessian matrix)\n",
    "\n",
    "$$\\nabla^2 f(x)} =\n",
    " \\begin{pmatrix}\n",
    "  \\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & ... & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\n",
    "  \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & ... & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\n",
    "  \\vdots & \\vdots & ... & \\vdots \\\\\n",
    "  \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} & ... & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\n",
    " \\end{pmatrix}"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "위와 같이 허세 행렬은 X의 두 원소에 대한 미분이다.  \n",
    "두 원소의 조합이 이루어지기 때문에 행렬 형태로 정의된다.\n",
    "\n",
    "기울기 $\\nabla f(x)$는 $\\frac{\\partial f}{\\partial X}$  \n",
    "헤세 행렬 $\\nabla^2 f(x)$는 $\\frac{\\partial^2 f}{\\partial X \\partial X^T}$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "$X \\leftarrow X - [\\nabla^2 f(x)]^{-1} \\nabla f'(x)$ 에서는 기울기와 헤세 행렬을 사용하여 X를 갱신했다.  \n",
    "X를 기울기 방향으로 갱신하고  \n",
    "그 진행 거리를 헤세 행렬의 역행렬을 사용하여 조정한다.  \n",
    "헤세 행렬이라는 2차 미분 정보를 이용함으로써 더 공격적으로 진행할 수 있어서 목적지에 더 빠르게 도달할 수 있는것이다.\n",
    "\n",
    "하지만 머신러닝, 신경망에서 잘 사용하지 않음"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 뉴턴 방법의 문제와 대안\n",
    "\n",
    "머신러닝에서 뉴턴 방법의 큰 단점  \n",
    "매개변수가 많아지면 헤세 행렬의 역행렬 계산에 자원이 너무 많이 소모된다.\n",
    "\n",
    "매개변수가 n개면 n^2만큼의 메모리를 사용  \n",
    "$nxn$의 역행렬 계산에는 n^3만큼 사용 \n",
    "\n",
    "대안 : 준 뉴턴 방법(QNM:Quasi-Netwon Method)  \n",
    "준 뉴턴 방법은 '헤세 행렬의 역행렬'을 근사해 사용하는 방법의 총칭(구체적인 방법이 존재하는 것은 아님)\n",
    "\n",
    "하지만 지금까지 딥러닝 분야에서 주류는 '**기울기만을 사용한 최적화**(SGD,Momentum,Adam등)'"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## double backprop의 용도: 헤세 행렬과 벡터의 곱\n",
    "\n",
    "double backprop의 사용 예 : **헤세 행렬과 벡터의 곱**계산  \n",
    "원소수가 늘어나면 헤세 행렬을 계산하는 비용이 매우 커짐.  \n",
    "그러나 헤세 행렬과 벡터의 곱의 '**결과**'만 필요하다면 double backprop을 사용하여 효율적으로 구할 수 있다.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "variable([[11 22 33]\n          [44 55 66]])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(np.array([[1,2,3],[4,5,6]]))\n",
    "c = Variable(np.array([[10,20,30],[40,50,60]]))\n",
    "y = x + c\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}